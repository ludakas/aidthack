{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: \"Warden P. Speech Commands: A public dataset for single-word speech recognition, 2017. Available from http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz\".\n",
    "\n",
    "Example code: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/speech_commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Define Imports\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os.path\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "\n",
    "# Modified from https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/speech_commands\n",
    "import input_data_ as input_data_\n",
    "import models_ as models_\n",
    "\n",
    "from mcfly import modelgen, find_architecture, storage\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, TensorBoard\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Bidirectional, Conv2D\n",
    "from keras.layers import Activation, Dropout, BatchNormalization, Flatten\n",
    "from keras.layers import MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.preprocessing.text import one_hot\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "wanted_words = 'yes,no,up,down,left,right,on,off,stop,go'\n",
    "sample_rate = 16000\n",
    "clip_duration_ms = 1000\n",
    "window_size_ms = 30.0\n",
    "window_stride_ms = 10.0\n",
    "dct_coefficient_count = 40\n",
    "\n",
    "#data_url = 'http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz'\n",
    "# data_url = None - to avoid downloading the data again\n",
    "data_url = None\n",
    "data_dir = 'speech_dataset/'\n",
    "data_dir_kaggle = 'data/test/'\n",
    "silence_percentage = 30.0\n",
    "unknown_percentage = 30.0\n",
    "validation_percentage = 10\n",
    "validation_percentage_kaggle = 0\n",
    "testing_percentage = 10\n",
    "testing_percentage_kaggle = 0\n",
    "\n",
    "time_shift_ms = 200.0\n",
    "learning_rate = 0.001\n",
    "\n",
    "batch_size = 1000\n",
    "n_epoch = 100\n",
    "tensorboard_dir='tensorboard/'\n",
    "model_dir = 'models/'\n",
    "\n",
    "# Automatically save checkpoints, if the validation accuracy is above the threshold\n",
    "best_checkpoint_path = 'best_checkpoints/'\n",
    "best_val_accuracy = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to get a batch generator\n",
    "def get_batches(x, y, batch_size=100):\n",
    "    while 1:\n",
    "        if len(x) >= batch_size:\n",
    "            yield x, y\n",
    "            x = []\n",
    "            y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data settings\n",
    "# We want to see all the logging messages for this tutorial.\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "# Start a new TensorFlow session.\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Begin by making sure we have the training data we need. If you already have\n",
    "# training data of your own, use `--data_url= ` on the command line to avoid\n",
    "# downloading.\n",
    "model_settings = models_.prepare_model_settings(\n",
    "    len(input_data_.prepare_words_list(wanted_words.split(','))),\n",
    "    sample_rate, clip_duration_ms, window_size_ms,\n",
    "    window_stride_ms, dct_coefficient_count)\n",
    "\n",
    "fingerprint_size = model_settings['fingerprint_size']\n",
    "label_count = model_settings['label_count']\n",
    "input_frequency_size = model_settings['dct_coefficient_count']\n",
    "input_time_size = model_settings['spectrogram_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build model\n",
    "cell_units = 1024\n",
    "activation = 'tanh'\n",
    "dropout = 0.1\n",
    "optimizer = 'adam'\n",
    "loss = 'categorical_crossentropy'\n",
    "model_name = 'birnn_gru'\n",
    "batch_norm = True\n",
    "\n",
    "#def ctc_loss(y_pred, y_true):\n",
    "#    with tf.name_scope(None):\n",
    "#        a_t = tf.constant(y_pred)\n",
    "#        idx = tf.where(tf.not_equal(a_t, 0))\n",
    "#        sparse = tf.SparseTensor(idx, tf.gather_nd(a_t, idx), a_t.get_shape())\n",
    "#        sequence_length = input_frequency_size * input_time_size\n",
    "#        loss = tf.nn.ctc_loss(sparse, y_true, sequence_length)\n",
    "#        return tf.reduce_mean(loss)\n",
    "    \n",
    "#loss = ctc_loss\n",
    "\n",
    "# Create a run_id in the following format\n",
    "# 'birnn_lstm-196_tanh_drop-0-5_batch-size-100_lrate-0-001_epoch-1000_batch-norm-True'\n",
    "run_id = '_'.join([model_name + '-' + str(cell_units),\n",
    "                  activation,\n",
    "                   'dp-' + str(dropout).replace('.', '-'),\n",
    "                   'bs-' + str(batch_size),\n",
    "                   'lr-' + str(learning_rate).replace('.', '-'),\n",
    "                   'ep-' + str(n_epoch),\n",
    "                   'bn-' + str(batch_norm),\n",
    "                   'unkn-' + str(int(unknown_percentage)),\n",
    "                   'sil-' + str(int(silence_percentage)),\n",
    "                   'ts-' + str(int(time_shift_ms))\n",
    "                  ])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Keras models\n",
    "def birnn_embedding(input_time_size, input_frequency_size, label_count, \n",
    "         cell_units=128, activation='tanh', dropout=0.5, batch_norm=False):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(label_count,cell_units))\n",
    "    model.add(Dropout(dropout))\n",
    "    if batch_norm:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Bidirectional(LSTM(cell_units, return_sequences=True,\n",
    "                                 activation=activation, dropout=dropout)))\n",
    "    if batch_norm:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Bidirectional(LSTM(cell_units, \n",
    "                                 activation=activation, dropout=dropout)))\n",
    "    if batch_norm:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Dense(label_count, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def cnn(input_time_size, input_frequency_size, label_count, \n",
    "         cell_units=128, activation='relu', dropout=0.5, batch_norm=False):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=2, activation=activation,\n",
    "                     input_shape=(input_time_size, input_frequency_size, 1)))\n",
    "    model.add(Conv2D(32, kernel_size=2, activation=activation))\n",
    "    model.add(Conv2D(32, kernel_size=2, activation=activation))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    if batch_norm:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Conv2D(48, kernel_size=2, activation=activation))\n",
    "    model.add(Conv2D(48, kernel_size=2, activation=activation))\n",
    "    model.add(Conv2D(48, kernel_size=2, activation=activation))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    if batch_norm:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Conv2D(64, kernel_size=2, activation=activation))\n",
    "    model.add(Conv2D(64, kernel_size=2, activation=activation))\n",
    "    model.add(Conv2D(64, kernel_size=2, activation=activation))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    if batch_norm:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    #model.add(Flatten())\n",
    "    model.add(Dense(cell_units, activation=activation))\n",
    "    if batch_norm:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(label_count, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def build_model(input_time_size, input_frequency_size, label_count, \n",
    "         model='birnn_embedding', cell_units=128, activation='tanh', dropout=0.5, batch_norm=False):\n",
    "    if 'birnn_embedding' in model:\n",
    "        return birnn_embedding(input_time_size, input_frequency_size, label_count, \n",
    "         cell_units, activation, dropout, batch_norm)\n",
    "    elif 'cnn' in model:\n",
    "        return cnn(input_time_size, input_frequency_size, label_count, \n",
    "         cell_units, activation, dropout, batch_norm)\n",
    "    else:\n",
    "        raise ValueError('Please provide a valid model')\n",
    "    \n",
    "optimizer = RMSprop()\n",
    "model = build_model(input_time_size, input_frequency_size, label_count, \n",
    "         model_name, cell_units, activation, dropout, batch_norm)\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tflearn models\n",
    "def lstm(input_time_size, input_frequency_size, label_count, \n",
    "         cell_units=128, activation='leaky_relu', dropout=0.5, batch_norm=False):\n",
    "    net = tflearn.input_data([None, input_time_size, input_frequency_size])\n",
    "    net = tflearn.lstm(net, cell_units, activation=activation, dropout=dropout)\n",
    "    if batch_norm:\n",
    "        net = tflearn.batch_normalization(net)\n",
    "    net = tflearn.fully_connected(net, label_count, activation='softmax')\n",
    "    return net\n",
    "\n",
    "def gru(input_time_size, input_frequency_size, label_count, \n",
    "         cell_units=128, activation='leaky_relu', dropout=0.5, batch_norm=False):\n",
    "    net = tflearn.input_data([None, input_time_size, input_frequency_size])\n",
    "    net = tflearn.gru(net, cell_units, activation=activation, dropout=dropout)\n",
    "    if batch_norm:\n",
    "        net = tflearn.batch_normalization(net)\n",
    "    net = tflearn.fully_connected(net, label_count, activation='softmax')\n",
    "    return net\n",
    "\n",
    "def birnn_lstm(input_time_size, input_frequency_size, label_count, \n",
    "         cell_units=128, activation='tanh', dropout=0.5, batch_norm=False):\n",
    "    net = tflearn.input_data([None, input_time_size, input_frequency_size])\n",
    "    net = tflearn.bidirectional_rnn(net,\n",
    "        tflearn.BasicLSTMCell(cell_units, activation=activation, batch_norm=batch_norm),\n",
    "        tflearn.BasicLSTMCell(cell_units, activation=activation, batch_norm=batch_norm))\n",
    "    net = tflearn.dropout(net, dropout)\n",
    "    net = tflearn.fully_connected(net, label_count, activation='softmax')\n",
    "    return net\n",
    "\n",
    "def birnn_gru(input_time_size, input_frequency_size, label_count, \n",
    "         cell_units=128, activation='tanh', dropout=0.5, batch_norm=False):\n",
    "    net = tflearn.input_data([None, input_time_size, input_frequency_size])\n",
    "    net = tflearn.bidirectional_rnn(net,\n",
    "        tflearn.GRUCell(cell_units, activation=activation),\n",
    "        tflearn.GRUCell(cell_units, activation=activation))\n",
    "    if batch_norm:\n",
    "        net = tflearn.batch_normalization(net)\n",
    "    net = tflearn.dropout(net, dropout)\n",
    "    net = tflearn.fully_connected(net, label_count, activation='softmax')\n",
    "    return net\n",
    "\n",
    "def cnn_birnn_gru(input_time_size, input_frequency_size, label_count, \n",
    "         cell_units=128, activation='tanh', dropout=0.5, batch_norm=False):\n",
    "    net = tflearn.input_data([None, input_time_size, input_frequency_size, 1])\n",
    "    net = tflearn.conv_2d(net, 32, 3, activation='relu')\n",
    "    net = tflearn.max_pool_2d(net, 2)\n",
    "    if batch_norm:\n",
    "        net = tflearn.batch_normalization(net)\n",
    "    net = tflearn.dropout(net, dropout)\n",
    "    net = tflearn.conv_2d(net, 64, 3, activation='relu')\n",
    "    net = tflearn.conv_2d(net, 64, 3, activation='relu')\n",
    "    net = tflearn.max_pool_2d(net, 2)\n",
    "    if batch_norm:\n",
    "        net = tflearn.batch_normalization(net)\n",
    "    net = tflearn.dropout(net, dropout)\n",
    "    net = tflearn.bidirectional_rnn(net,\n",
    "        tflearn.GRUCell(cell_units, activation=activation),\n",
    "        tflearn.GRUCell(cell_units, activation=activation))\n",
    "    if batch_norm:\n",
    "        net = tflearn.batch_normalization(net)\n",
    "    net = tflearn.dropout(net, dropout)\n",
    "    network = fully_connected(network, cell_units, activation='relu')\n",
    "    network = dropout(network, dropout)\n",
    "    net = tflearn.fully_connected(net, label_count, activation='softmax')\n",
    "    return net\n",
    "\n",
    "def build_model(input_time_size, input_frequency_size, label_count, \n",
    "         model='birnn', cell_units=128, activation='tanh', dropout=0.5, batch_norm=False):\n",
    "    if 'birnn_lstm' in model:\n",
    "        return birnn_lstm(input_time_size, input_frequency_size, label_count, \n",
    "         cell_units, activation, dropout, batch_norm)\n",
    "    elif 'birnn_gru' in model:\n",
    "        return birnn_gru(input_time_size, input_frequency_size, label_count, \n",
    "         cell_units, activation, dropout, batch_norm)\n",
    "    elif 'lstm' in model:\n",
    "        return lstm(input_time_size, input_frequency_size, label_count, \n",
    "         cell_units, activation, dropout, batch_norm)\n",
    "    elif 'gru' in model:\n",
    "        return gru(input_time_size, input_frequency_size, label_count, \n",
    "         cell_units, activation, dropout, batch_norm)\n",
    "    elif 'cnn_birnn_gru' in model:\n",
    "        return cnn_birnn_gru(input_time_size, input_frequency_size, label_count, \n",
    "         cell_units, activation, dropout, batch_norm)\n",
    "    else:\n",
    "        raise ValueError('Please provide a valid model')\n",
    "\n",
    "# Bidirectional RNN\n",
    "net = build_model(input_time_size, input_frequency_size, label_count, \n",
    "         model_name, cell_units, activation, dropout, batch_norm)\n",
    "net = tflearn.regression(net, optimizer=optimizer, learning_rate=learning_rate, loss=loss)\n",
    "\n",
    "model = tflearn.DNN(net, tensorboard_verbose=0, tensorboard_dir=tensorboard_dir,\n",
    "            best_checkpoint_path=best_checkpoint_path, best_val_accuracy=best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare training and validation data\n",
    "audio_processor_training = input_data_.AudioProcessor(\n",
    "    data_url, data_dir, silence_percentage,\n",
    "    unknown_percentage,\n",
    "    wanted_words.split(','), validation_percentage,\n",
    "    testing_percentage, model_settings)\n",
    "\n",
    "# Get validation data\n",
    "validation_fingerprints, validation_ground_truth = audio_processor_training.get_data(-1, 0, model_settings, 0.0,\n",
    "                             0.0, 0, 'validation', sess)\n",
    "# Turn validation data into a batch generator\n",
    "validation_fingerprints, validation_ground_truth = next(get_batches(validation_fingerprints, validation_ground_truth, batch_size))\n",
    "\n",
    "# Reshape validation data to match the model input\n",
    "validation_fingerprints = np.reshape(validation_fingerprints, (-1, input_time_size, input_frequency_size))\n",
    "\n",
    "def get_training_data(audio_processor_training, model_settings, sess):\n",
    "    time_shift_samples = np.random.uniform(0.5, 1.5) * int((time_shift_ms * sample_rate) / 1000)\n",
    "    background_frequency = np.random.uniform(0.5, 1.0)\n",
    "    background_volume = np.random.uniform(0.0, 0.5)\n",
    "    # Pull the audio samples we'll use for training.\n",
    "    # Get training data\n",
    "    train_fingerprints, train_ground_truth = audio_processor_training.get_data(\n",
    "        -1, 0, model_settings, background_frequency,\n",
    "        background_volume, time_shift_samples, 'training', sess)\n",
    "    # Turn training data into a batch generator\n",
    "    train_fingerprints, train_ground_truth = next(get_batches(train_fingerprints, train_ground_truth, batch_size))\n",
    "    # Reshape training data to match the model input\n",
    "    train_fingerprints = np.reshape(train_fingerprints, (-1, input_time_size, input_frequency_size))\n",
    "    \n",
    "    return train_fingerprints, train_ground_truth"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_callbacks(filepath, patience=5):\n",
    "    es = EarlyStopping('val_loss', patience=patience, mode=\"min\")\n",
    "    msave = ModelCheckpoint(filepath, save_best_only=True)\n",
    "    tb = TensorBoard(tensorboard_dir + run_id)\n",
    "    return [es, msave, tb]\n",
    "\n",
    "callbacks = get_callbacks(best_checkpoint_path + run_id, patience=20)\n",
    "\n",
    "# Training Keras\n",
    "for i in range(n_epoch):\n",
    "    print('epoch: ' + str(i))\n",
    "    train_fingerprints, train_ground_truth = get_training_data(audio_processor_training,\n",
    "        model_settings, sess)\n",
    "    history = model.fit(train_fingerprints, train_ground_truth, epochs=1,\n",
    "              validation_data=(validation_fingerprints, validation_ground_truth),\n",
    "              batch_size=batch_size, callbacks=callbacks)\n",
    "    \n",
    "model.save(model_dir + run_id)\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2011  | total loss: \u001b[1m\u001b[32m0.27628\u001b[0m\u001b[0m | time: 17.717s\n",
      "\u001b[2K\r",
      "| Adam | epoch: 068 | loss: 0.27628 - acc: 0.9381 -- iter: 01000/29662\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-703ae1354f47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     model.fit(train_fingerprints, train_ground_truth, n_epoch=1,\n\u001b[1;32m      7\u001b[0m               \u001b[0mvalidation_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_fingerprints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_ground_truth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m               show_metric=True, batch_size=batch_size, run_id=run_id)\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrun_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tflearn/models/dnn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_inputs, Y_targets, n_epoch, validation_set, show_metric, batch_size, shuffle, snapshot_epoch, snapshot_step, excl_trainops, validation_batch_size, run_id, callbacks)\u001b[0m\n\u001b[1;32m    214\u001b[0m                          \u001b[0mexcl_trainops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexcl_trainops\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                          \u001b[0mrun_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                          callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tflearn/helpers/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, feed_dicts, n_epoch, val_feed_dicts, show_metric, snapshot_step, snapshot_epoch, shuffle_all, dprep_dict, daug_dict, excl_trainops, run_id, callbacks)\u001b[0m\n\u001b[1;32m    336\u001b[0m                                                        \u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0msnapshot_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m                                                        \u001b[0msnapshot_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m                                                        show_metric)\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m                             \u001b[0;31m# Update training state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tflearn/helpers/trainer.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, training_step, snapshot_epoch, snapshot_step, show_metric)\u001b[0m\n\u001b[1;32m    815\u001b[0m         \u001b[0mtflearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         _, train_summ_str = self.session.run([self.train, self.summ_op],\n\u001b[0;32m--> 817\u001b[0;31m                                              feed_batch)\n\u001b[0m\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m         \u001b[0;31m# Retrieve loss value from summary string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training tflearn\n",
    "tflearn.is_training(True)\n",
    "for i in range(n_epoch):\n",
    "    train_fingerprints, train_ground_truth = get_training_data(audio_processor_training,\n",
    "        model_settings, sess)\n",
    "    model.fit(train_fingerprints, train_ground_truth, n_epoch=1,\n",
    "              validation_set=(validation_fingerprints, validation_ground_truth),\n",
    "              show_metric=True, batch_size=batch_size, run_id=run_id)\n",
    "    \n",
    "model.save(model_dir + run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/sladomic/Projects/TensorFlow Speech Recognition Challenge/best_checkpoints/9535\n",
      "INFO:tensorflow:Final test accuracy = 95.8% (N=4109)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "set_size = audio_processor_training.set_size('testing')\n",
    "total_accuracy = 0\n",
    "total_conf_matrix = None\n",
    "\n",
    "test_fingerprints, test_ground_truth = audio_processor_training.get_data(\n",
    "        -1, 0, model_settings, 0.0, 0.0, 0, 'testing', sess)\n",
    "test_fingerprints, test_ground_truth = next(get_batches(test_fingerprints, test_ground_truth, batch_size))\n",
    "test_fingerprints = np.reshape(test_fingerprints, (-1, input_time_size, input_frequency_size))\n",
    "\n",
    "tflearn.is_training(False)\n",
    "#model.load(model_dir + run_id)\n",
    "model.load(best_checkpoint_path + '9535')\n",
    "#model = load_model(model_dir + run_id)\n",
    "total_accuracy = model.evaluate(test_fingerprints, test_ground_truth, batch_size=batch_size)\n",
    "tf.logging.info('Final test accuracy = %.1f%% (N=%d)' % (total_accuracy[0] * 100,\n",
    "                                                       set_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict on the Kaggle test set\n",
    "audio_processor_predict = input_data_.AudioProcessor(\n",
    "    data_url, data_dir_kaggle, silence_percentage,\n",
    "    unknown_percentage,\n",
    "    wanted_words.split(','), validation_percentage_kaggle,\n",
    "    testing_percentage_kaggle, model_settings)\n",
    "\n",
    "predict_fingerprints, predict_ground_truth = audio_processor_predict.get_data(\n",
    "        -1, 0, model_settings, 0.0, 0.0, 0, 'training', sess, False)\n",
    "predict_fingerprints, predict_ground_truth = next(get_batches(predict_fingerprints, predict_ground_truth, batch_size))\n",
    "predict_fingerprints = np.reshape(predict_fingerprints, (-1, input_time_size, input_frequency_size))\n",
    "\n",
    "tflearn.is_training(False)\n",
    "model.load(model_dir + run_id)\n",
    "y_ = model.predict(predict_fingerprints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'audio_processor_training' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a33d73133f78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Export predictions to csv for submission\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwords_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio_processor_training\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mword_to_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio_processor_training\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_to_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msilence_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_silence_'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mindex_to_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords_list\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'unknown'\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_to_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'audio_processor_training' is not defined"
     ]
    }
   ],
   "source": [
    "# Export predictions to csv for submission\n",
    "words_list = audio_processor_training.words_list\n",
    "word_to_index = audio_processor_training.word_to_index\n",
    "silence_idx = word_to_index['_silence_']\n",
    "index_to_word = {y:x if x in words_list else 'unknown' for x,y in word_to_index.items()}\n",
    "index_to_word[silence_idx] = 'silence'\n",
    "y_labels = [np.argmax(x) for x in y_]\n",
    "y_names = [index_to_word[x] for x in y_labels]\n",
    "data_index = audio_processor_predict.data_index\n",
    "with open('submissions/submission_' + run_id + '.csv', 'w') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter=',')\n",
    "    spamwriter.writerow(['fname', 'label'])\n",
    "    for i in range(len(y_names)):\n",
    "        spamwriter.writerow([os.path.basename(data_index['training'][i]['file']), y_names[i]])\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas\n",
    "    - [ ] seq2seq\n",
    "    - [x] bidirectional RNN\n",
    "    - [x] more units\n",
    "    - [ ] https://arxiv.org/pdf/1710.04515.pdf\n",
    "    - [ ] Capsule network\n",
    "    - [ ] softmax_categorical_crossentropy\n",
    "    - [ ] AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(model_dir + run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "audio_processor_predict = input_data_.AudioProcessor(\n",
    "    data_url, data_dir_kaggle, silence_percentage,\n",
    "    unknown_percentage,\n",
    "    wanted_words.split(','), validation_percentage_kaggle,\n",
    "    testing_percentage_kaggle, model_settings)\n",
    "\n",
    "predict_fingerprints, predict_ground_truth = audio_processor_predict.get_data(\n",
    "        -1, 0, model_settings, 0.0, 0.0, 0, 'training', sess, False)\n",
    "predict_fingerprints, predict_ground_truth = next(get_batches(predict_fingerprints, predict_ground_truth, batch_size))\n",
    "predict_fingerprints = np.reshape(predict_fingerprints, (-1, input_time_size, input_frequency_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/sladomic/Projects/TensorFlow Speech Recognition Challenge/best_checkpoints/9535\n",
      "(158538, 12)\n"
     ]
    }
   ],
   "source": [
    "tflearn.is_training(False)\n",
    "model.load(best_checkpoint_path + '9535')\n",
    "y_ = []\n",
    "for i in range(0, len(predict_fingerprints), 1000):\n",
    "    y_.append(model.predict(predict_fingerprints[i:i+1000]))\n",
    "y2 = []\n",
    "for x in y_:\n",
    "    for i in x:\n",
    "        y2.append(i)\n",
    "print(np.array(y2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready\n"
     ]
    }
   ],
   "source": [
    "y_ = y2\n",
    "print(\"Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
